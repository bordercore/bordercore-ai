"""
This module defines a FunctionCall class responsible for invoking dynamically
imported Python functions based on structured tool calls generated by a language model.
It parses LLM responses containing JSON payloads, dispatches them to appropriate tools,
and integrates the results back into the chat context for further inference.
"""

import importlib
import json
import random
import re
import string
from typing import Any, Dict, List, Optional

from modules.exceptions import JsonParsingError, LLMResponseError


class FunctionCall():
    """
    Handles function tool calls generated by an LLM, including dynamic function dispatch,
    JSON parsing, argument translation, and integration with chatbot messaging flow.
    """

    def __init__(self, model_name: str, **args: Any) -> None:
        """
        Initialize FunctionCall with a model name and additional configuration.

        Args:
            model_name: The name of the model to use for follow-up inference.
            **args: Additional attributes, such as tool_name and tool_list.
        """
        self.model_name = model_name
        self.args = args
        self.tool_name: Optional[str] = args.get("tool_name")
        self.tool_list: Optional[str] = args.get("tool_list")

    def generate_random_id(self, length: int = 6) -> str:
        """
        Generate a random alphanumeric string for tool_call IDs.

        Args:
            length: Length of the generated ID string.

        Returns:
            A random string of the specified length.
        """
        characters = string.ascii_letters + string.digits
        random_id = "".join(random.choice(characters) for _ in range(length))
        return random_id

    def rename_key(self, data: Dict[str, Any], old_key: str, new_key: str) -> None:
        """
        Rename a key in a dictionary if it exists.

        Args:
            data: The dictionary to update.
            old_key: The existing key name.
            new_key: The new key name.
        """
        if old_key in data:
            data[new_key] = data.pop(old_key)

    def call_function_from_json(self, messages: List[Dict[str, Any]], response: str) -> str:
        """
        Parse a function call from a model-generated response, call the corresponding tool,
        and return the model's final response.

        Args:
            messages: The current chat history.
            response: A string from the model containing a tool call in special JSON format.

        Returns:
            The final content string returned from the model after invoking the tool.
        """
        json_match = re.search(r"<\|python_tag\|>(.*?)<\|eom_id\|>", response)

        if not json_match:
            raise LLMResponseError(f"No JSON found in the LLM response: {response}")

        tool_call = None
        json_string = json_match.group(1)
        try:
            tool_call = json.loads(json_string)
        except json.JSONDecodeError as e:
            raise JsonParsingError(f"Error decoding JSON: {e}") from e

        tool_call_id = self.generate_random_id()

        messages.append(
            {
                "role": "assistant",
                "tool_calls": [
                    {
                        "id": tool_call_id,
                        "type": "function",
                        "function": tool_call
                    }
                ]
            }
        )

        # The chat template expects "arguments", but Llama-3.1 produces "parameters"
        #  instead. We need to change this to avoid this Jinja error:
        #  TypeError: Object of type Undefined is not JSON serializable
        self.rename_key(messages[-1]["tool_calls"][0]["function"], "parameters", "arguments")

        func_name = tool_call["name"]
        parameters = tool_call["arguments"]

        main_module = importlib.import_module(f"modules.{self.tool_name}")

        if not hasattr(main_module, func_name):
            raise AttributeError(f"Function '{func_name}' not found in module '{self.tool_name}'")

        func = getattr(main_module, func_name)

        if not callable(func):
            raise TypeError(f"'{func_name}' is not callable")

        result = func(**parameters)

        messages.append(
            {
                "role": "tool",
                "tool_call_id": tool_call_id,
                "name": func_name,
                "content": str(result)
            }
        )

        from modules.chatbot import ChatBot
        chatbot = ChatBot(self.model_name, temperature=0.1)
        response = chatbot.send_message_to_model(messages, replace_context=True, tool_name=self.tool_name, tool_list=self.tool_list)
        content = ChatBot.get_streaming_message(response)

        # Remove trailing <|eot_id|> token.
        eot_id = "<|eot_id|>"
        if content.endswith(eot_id):
            content = content[:-len(eot_id)]

        return content

    def choose_function(self, messages: List[Dict[str, Any]]) -> str:
        """
        Ask the model to decide which tool function to call, given the chat history.

        Args:
            messages: The chat message history.

        Returns:
            A string response containing the model's tool function decision.
        """
        from modules.chatbot import ChatBot
        chatbot = ChatBot(self.model_name)
        response = chatbot.send_message_to_model(messages, replace_context=True, tool_name=self.tool_name, tool_list=self.tool_list)
        return ChatBot.get_streaming_message(response)

    def run(self, prompt: str) -> str:
        """
        Executes a tool-assisted prompt by choosing and invoking a function.

        Args:
            prompt: The user-supplied natural language question or command.

        Returns:
            A string containing the final model-generated response after function invocation.
        """
        prompt += f"{prompt} Please don't tell me how you got the answer, I only want the answer."
        messages = [
            {"role": "system", "content": "You are a helpful assistant that can use functions when necessary. When you receive a tool call response, use the output to format an answer to the orginal user question."},
            {"role": "user", "content": prompt}
        ]

        target_function = self.choose_function(messages)
        return "Using Wolfram Alpha. " + self.call_function_from_json(messages, target_function)
